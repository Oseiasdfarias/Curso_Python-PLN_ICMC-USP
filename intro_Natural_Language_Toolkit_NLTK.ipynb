{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro-Natural_Language_Toolkit_NLTK.ipynb",
      "provenance": [],
      "mount_file_id": "1nj5gINxlOJD4i7E8_zQoXwfE-e-LHVTf",
      "authorship_tag": "ABX9TyO58QITMYF2LeMIfhfXnY9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oseiasdfarias/Curso_Python-PLN_ICMC-USP/blob/main/intro_Natural_Language_Toolkit_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqtwfvvAweVJ"
      },
      "source": [
        "# **Python para Processamento de Linguagem Natural ICMC|USP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGa5crM_wRqj"
      },
      "source": [
        "## **Instalando o NLTK**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEdUVJmYwDmi",
        "outputId": "2765097b-781c-4f15-ada5-e15f542982df"
      },
      "source": [
        "! pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r24Ej-2Ew-ed"
      },
      "source": [
        "## Baixando dados extras (Windows)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm1jX16UwyK7"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHbQKD5ExPkX"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuIr1m3j1MLH"
      },
      "source": [
        "\n",
        "## **Primeiros Passos com o NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-vJSopg0TGF"
      },
      "source": [
        "### **Listar as palavras existente no Corpus Mac-Morpho**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_hMzgJSyakM",
        "outputId": "2200d8b1-5842-4914-999a-0d2410b64979"
      },
      "source": [
        "nltk.corpus.mac_morpho.words()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jersei', 'atinge', 'média', 'de', 'Cr$', '1,4', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgSdAGgO06I6"
      },
      "source": [
        "### **Listar uma ou mais palavras existente no Corpus Mac-Morpho**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VuT8SQtm1AWT",
        "outputId": "edbc41a1-4e8c-40b3-8151-62b8a722b983"
      },
      "source": [
        "nltk.corpus.mac_morpho.words()[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Jersei'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUU_xEbL1DW_",
        "outputId": "ea93b749-fa62-4703-eb7c-e79ea2b2736f"
      },
      "source": [
        "nltk.corpus.mac_morpho.words()[3:7]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['de', 'Cr$', '1,4', 'milhão']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Ioqry00eGO"
      },
      "source": [
        "### **Listar sentenças existente no Corpus Mac-Morpho**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxcmlVtRz0UQ",
        "outputId": "6ff88808-ab7d-4265-d73e-43c228416fac"
      },
      "source": [
        "nltk.corpus.mac_morpho.sents()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Jersei', 'atinge', 'média', 'de', 'Cr$', '1,4', 'milhão', 'em', 'a', 'venda', 'de', 'a', 'Pinhal', 'em', 'São', 'Paulo'], ['Programe', 'sua', 'viagem', 'a', 'a', 'Exposição', 'Nacional', 'do', 'Zebu', ',', 'que', 'começa', 'dia', '25'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wx3SmTm0mhq"
      },
      "source": [
        "### **Listar uma ou mais sentença existente no Corpus Mac-Morpho**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdASeLxk0Nq2",
        "outputId": "632d8459-d5cb-425c-8950-b2a718b786cf"
      },
      "source": [
        "nltk.corpus.mac_morpho.sents()[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jersei',\n",
              " 'atinge',\n",
              " 'média',\n",
              " 'de',\n",
              " 'Cr$',\n",
              " '1,4',\n",
              " 'milhão',\n",
              " 'em',\n",
              " 'a',\n",
              " 'venda',\n",
              " 'de',\n",
              " 'a',\n",
              " 'Pinhal',\n",
              " 'em',\n",
              " 'São',\n",
              " 'Paulo']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCBnskde0vQI",
        "outputId": "819e86be-c9a9-4be9-f2ba-0b25ac46b008"
      },
      "source": [
        "nltk.corpus.mac_morpho.sents()[0:3]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Jersei',\n",
              "  'atinge',\n",
              "  'média',\n",
              "  'de',\n",
              "  'Cr$',\n",
              "  '1,4',\n",
              "  'milhão',\n",
              "  'em',\n",
              "  'a',\n",
              "  'venda',\n",
              "  'de',\n",
              "  'a',\n",
              "  'Pinhal',\n",
              "  'em',\n",
              "  'São',\n",
              "  'Paulo'],\n",
              " ['Programe',\n",
              "  'sua',\n",
              "  'viagem',\n",
              "  'a',\n",
              "  'a',\n",
              "  'Exposição',\n",
              "  'Nacional',\n",
              "  'do',\n",
              "  'Zebu',\n",
              "  ',',\n",
              "  'que',\n",
              "  'começa',\n",
              "  'dia',\n",
              "  '25'],\n",
              " ['Safra',\n",
              "  'recorde',\n",
              "  'e',\n",
              "  'disponibilidade',\n",
              "  'de',\n",
              "  'crédito',\n",
              "  'ativam',\n",
              "  'vendas',\n",
              "  'de',\n",
              "  'máquinas',\n",
              "  'agrícolas']]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGxDbvLu1xL4"
      },
      "source": [
        "### **Acessando as palavras com suas etiquetas(targs)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocpX0eYK0zkG",
        "outputId": "4e8617aa-bbe8-49b9-9647-48785ea0b6ba"
      },
      "source": [
        "nltk.corpus.mac_morpho.tagged_words()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84zTHz-C2PUy"
      },
      "source": [
        "### **Acessando os sentenças com suas etiquetas(targs)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-akemswU2EjW",
        "outputId": "65a3504f-0770-4c57-a090-80670ecdb6bf"
      },
      "source": [
        "nltk.corpus.mac_morpho.tagged_sents()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ('de', 'PREP'), ('Cr$', 'CUR'), ('1,4', 'NUM'), ('milhão', 'N'), ('em', 'PREP|+'), ('a', 'ART'), ('venda', 'N'), ('de', 'PREP|+'), ('a', 'ART'), ('Pinhal', 'NPROP'), ('em', 'PREP'), ('São', 'NPROP'), ('Paulo', 'NPROP')], [('Programe', 'V'), ('sua', 'PROADJ'), ('viagem', 'N'), ('a', 'PREP|+'), ('a', 'ART'), ('Exposição', 'NPROP'), ('Nacional', 'NPROP'), ('do', 'NPROP'), ('Zebu', 'NPROP'), (',', ','), ('que', 'PRO-KS-REL'), ('começa', 'V'), ('dia', 'N'), ('25', 'N|AP')], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9C-9DYZ2ioM",
        "outputId": "b39480f0-f084-497c-f620-7d33ca59bb53"
      },
      "source": [
        "nltk.corpus.mac_morpho.tagged_sents()[0]\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Jersei', 'N'),\n",
              " ('atinge', 'V'),\n",
              " ('média', 'N'),\n",
              " ('de', 'PREP'),\n",
              " ('Cr$', 'CUR'),\n",
              " ('1,4', 'NUM'),\n",
              " ('milhão', 'N'),\n",
              " ('em', 'PREP|+'),\n",
              " ('a', 'ART'),\n",
              " ('venda', 'N'),\n",
              " ('de', 'PREP|+'),\n",
              " ('a', 'ART'),\n",
              " ('Pinhal', 'NPROP'),\n",
              " ('em', 'PREP'),\n",
              " ('São', 'NPROP'),\n",
              " ('Paulo', 'NPROP')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Wtwb7K3_tI"
      },
      "source": [
        "### **Tokenizando textos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5Zzy7AI2qBI"
      },
      "source": [
        "texto = \"\"\"Com um passe de Eli Manning para Plaxico Burress a 39 \n",
        "segundos do fim, o New York Giants anotou o touchdown decisivo e \n",
        "derrubou o favorito New England Patriots por 17 a 14 neste domingo, \n",
        "em Glendale, no Super Bowl XLII.\"\"\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjibFe0-4MVW",
        "outputId": "9e59049e-659f-4de4-8796-495c881e86f1"
      },
      "source": [
        "nltk.word_tokenize(texto)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Com',\n",
              " 'um',\n",
              " 'passe',\n",
              " 'de',\n",
              " 'Eli',\n",
              " 'Manning',\n",
              " 'para',\n",
              " 'Plaxico',\n",
              " 'Burress',\n",
              " 'a',\n",
              " '39',\n",
              " 'segundos',\n",
              " 'do',\n",
              " 'fim',\n",
              " ',',\n",
              " 'o',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Giants',\n",
              " 'anotou',\n",
              " 'o',\n",
              " 'touchdown',\n",
              " 'decisivo',\n",
              " 'e',\n",
              " 'derrubou',\n",
              " 'o',\n",
              " 'favorito',\n",
              " 'New',\n",
              " 'England',\n",
              " 'Patriots',\n",
              " 'por',\n",
              " '17',\n",
              " 'a',\n",
              " '14',\n",
              " 'neste',\n",
              " 'domingo',\n",
              " ',',\n",
              " 'em',\n",
              " 'Glendale',\n",
              " ',',\n",
              " 'no',\n",
              " 'Super',\n",
              " 'Bowl',\n",
              " 'XLII',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYwoRriE5ndH"
      },
      "source": [
        "### **Regexp no NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sU805_28PDV"
      },
      "source": [
        "### **Retirando pontuações do texto com o Módulo RegexpTokenizer do NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSZyoPjx4WGF"
      },
      "source": [
        "from nltk import RegexpTokenizer"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTynLaHA6An1"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(texto) "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvBZ2l__7PXg",
        "outputId": "1f5a6546-0703-4530-d23e-f683244934fd"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Com',\n",
              " 'um',\n",
              " 'passe',\n",
              " 'de',\n",
              " 'Eli',\n",
              " 'Manning',\n",
              " 'para',\n",
              " 'Plaxico',\n",
              " 'Burress',\n",
              " 'segundos',\n",
              " 'do',\n",
              " 'fim',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Giants',\n",
              " 'anotou',\n",
              " 'touchdown',\n",
              " 'decisivo',\n",
              " 'derrubou',\n",
              " 'favorito',\n",
              " 'New',\n",
              " 'England',\n",
              " 'Patriots',\n",
              " 'por',\n",
              " 'neste',\n",
              " 'domingo',\n",
              " 'em',\n",
              " 'Glendale',\n",
              " 'no',\n",
              " 'Super',\n",
              " 'Bowl',\n",
              " 'XLII']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl3LNU758oVO"
      },
      "source": [
        "### **Retirando pontuações e números do texto com o Módulo RegexpTokenizer do NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBuQSvyC7Z0L",
        "outputId": "ed33e83e-f7ce-4448-aefc-f1ed94e3fdb3"
      },
      "source": [
        "  tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w+')\n",
        "  tokens = tokenizer.tokenize(texto)\n",
        "  tokens"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Com',\n",
              " 'um',\n",
              " 'passe',\n",
              " 'de',\n",
              " 'Eli',\n",
              " 'Manning',\n",
              " 'para',\n",
              " 'Plaxico',\n",
              " 'Burress',\n",
              " 'segundos',\n",
              " 'do',\n",
              " 'fim',\n",
              " 'New',\n",
              " 'York',\n",
              " 'Giants',\n",
              " 'anotou',\n",
              " 'touchdown',\n",
              " 'decisivo',\n",
              " 'derrubou',\n",
              " 'favorito',\n",
              " 'New',\n",
              " 'England',\n",
              " 'Patriots',\n",
              " 'por',\n",
              " 'neste',\n",
              " 'domingo',\n",
              " 'em',\n",
              " 'Glendale',\n",
              " 'no',\n",
              " 'Super',\n",
              " 'Bowl',\n",
              " 'XLII']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo_bkblV8IDi"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGiNKIIQ9X99"
      },
      "source": [
        "### **Verificando a Frequência de palavras em um texto**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOhoV2VN9f3A",
        "outputId": "5d4ef77b-705a-4880-bbf2-6ae8a1d32a69"
      },
      "source": [
        "tokens = nltk.word_tokenize(texto)\n",
        "frequencia = nltk.FreqDist(tokens)\n",
        "frequencia.most_common(10)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 3),\n",
              " ('o', 3),\n",
              " ('a', 2),\n",
              " ('New', 2),\n",
              " ('Com', 1),\n",
              " ('um', 1),\n",
              " ('passe', 1),\n",
              " ('de', 1),\n",
              " ('Eli', 1),\n",
              " ('Manning', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCTdIAnlAIBk"
      },
      "source": [
        "### **Trabalhando com textos maiores**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHa-4AHx-LW7"
      },
      "source": [
        "corpus = open(\"/content/drive/MyDrive/Curso_PLN_USP/arquivo2.txt\").read()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcOgZggjAWPh",
        "outputId": "ce41fb76-6154-486a-b8ff-f73612f1b302"
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Giants batem os Patriots no Super Bowl XLII\n",
            "Azarões acabam com a invencibilidade de New England e ficam com o título da temporada\n",
            "04/02/2008 - 01h07m - Atualizado em 04/02/2008 - 09h49m\n",
            "\n",
            "Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII. O resultado, uma das maiores zebras da história do Super Bowl, acabou com a temporada perfeita de Tom Brady e companhia, que esperavam fazer história ao levantar o troféu da NFL sem sofrer uma derrota no ano. \n",
            "\n",
            "A vitória dos Giants, porém, também ficará para a história. Pela primeira vez, irmãos quarterbacks triunfam no Super Bowl em temporadas consecutivas. No ano passado, Peyton Manning, irmão de Eli, chegou ao título máximo da NFL pelo Indianapolis Colts.\n",
            "\n",
            "A partida\n",
            "\n",
            "Os Giants começaram com a posse de bola, e mostraram logo que iriam alongar ao máximo suas posses de bola. Misturando corridas com Brandon Jacobs e passes curtos, o time de Nova York chegou à red zone logo na primeira campanha. O avanço, no entanto, parou na linha de 17 jardas e Lawrence Tynes converteu o field goal de 32 jardas para abrir o placar.\n",
            "\n",
            "Eli Manning e companhia ficaram 9m54s com a bola, mas o ataque dos Patriots não entrou em campo frio. Logo no retorno do kickoff, o running back Laurence Maroney avançou 43 jardas, deixando Tom Brady em boa posição. Com passes curtos, os Patriots chegaram à linha de 17 jardas e, graças a uma penalidade (interferência de passe) do linebacker Antonio Pierce, alcançaram a linha de uma jarda. Maroney avançou pelo chão e anotou o primeiro touchdown do jogo.\n",
            "\n",
            "Os Giants pareciam rumo à virada na campanha seguinte. Manning achou Amani Toomer para um avanço de 38 jardas, e o time de Nova York entrou novamente na red zone. Com a bola na linha de 14 jardas dos Patriots, os Giants sofreram um revés. Manning passou para Steve Smith, que soltou a bola. Ellis Hobbs aproveitou, tomou a posse para os Patriots, e avançou 23 jardas. \n",
            "\n",
            "A defesa de Nova York manteve o jogo equilibrado. Com dois sacks seguidos, os Giants forçaram o punt e recuperaram a bola. Mas a campanha seguinte provou ser outra decepção para Nova York. O time chegou à linha de 25 jardas, mas Manning sofreu um sack e cometeu um fumble, e o ataque voltou para a linha de 39 jardas, não conseguindo pontuar mais uma vez.\n",
            "\n",
            "Os Patriots tiveram uma última chance de marcar antes do intervalo, mas, a 22 segundos do fim do segundo período, Brady foi novamente sacado. Desta vez, ele cometeu o fumble e os Giants tomaram a posse de bola. Manning tentou um passe longo, de 50 jardas, nos últimos segundos, mas não teve sucesso. \n",
            "\n",
            "O jogo continuou amarrado no terceiro quarto, com as defesas levando a melhor sobre os ataques. A única chance de pontuar do período foi dos Patriots, que chegaram à linha de 31 jardas dos Giants. O técnico Bill Bellichick, porém, optou por uma quarta descida em vez de um field goal. Brady tentou um passe para Jabar Gaffney, mas não conseguiu completar.\n",
            "\n",
            "O último período começou arrasador para os Giants. na primeira jogada, Manning achou o tight end Kevin Boss, para um incrível avanço de 45 jardas, que deixou o time na linha de 35 dos Patriots. Outro lançamento, desta vez para Steve Smith, marcou o avanço até a linha de 12 jardas. Duas jogadas depois, David Tyree pegou um passe de cinco jardas na end zone para anotar o touchdown e virar o jogo.\n",
            "\n",
            "Na hora da decisão, o ataque dos Patriots voltou a funcionar. Com uma série de passes curtos e variados, Brady achou Wes Welker, Randy Moss e Kevin Faulk seguidas vezes até chegar à red zone. A 2m45s do fim, o quarterback conectou mais uma vez com Moss, que se desmarcou e ficou livre na lateral direita da end zone.\n",
            "\n",
            "Quando os fãs de New England já comemoravam a vitória, o inesperado aconteceu. Em uma jogada incrível, Eli Manning se soltou de dois marcadores que o seguravam pela camisa e, na corrida, lançou para Amani Toomer. O wide receiver, bem marcado, saltou e conseguiu a fazer recepção para um avanço de 32 jardas, deixando os Giants na linha de 24 de New England.\n",
            "\n",
            "Quatro jogadas depois, a 39 segundos do fim, Manning achou Plaxico Burress na end zone para conseguir o touchdown do título.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww1wGwPTAZfe",
        "outputId": "7b1e05f5-a580-4c22-c28e-eaceddc34838"
      },
      "source": [
        "  # Cria uma regex que pega apenas as palavras de um texto\n",
        "  tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w+')\n",
        "  tokens = tokenizer.tokenize(corpus)\n",
        "\n",
        "\n",
        "  # transformando as palavras em minúsculas\n",
        "  nova_lista = []\n",
        "  for token in tokens:\n",
        "    nova_lista.append(token.lower())\n",
        "\n",
        "\n",
        "  # Conta a quantidade de vezes que a palavra se repete no texto\n",
        "  frequencia = nltk.FreqDist(nova_lista)\n",
        "  frequencia.most_common(10)\n",
        "\n",
        "  "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 34),\n",
              " ('para', 16),\n",
              " ('jardas', 15),\n",
              " ('os', 13),\n",
              " ('com', 13),\n",
              " ('na', 13),\n",
              " ('do', 12),\n",
              " ('giants', 11),\n",
              " ('um', 11),\n",
              " ('patriots', 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t06ZUsSRRaB"
      },
      "source": [
        "### **Retirando palavras irrelevantes do tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da22-LscQ3eC",
        "outputId": "7e754c3c-ba5a-41b8-8fe5-e57b24f4a009"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "stopwords"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['de',\n",
              " 'a',\n",
              " 'o',\n",
              " 'que',\n",
              " 'e',\n",
              " 'é',\n",
              " 'do',\n",
              " 'da',\n",
              " 'em',\n",
              " 'um',\n",
              " 'para',\n",
              " 'com',\n",
              " 'não',\n",
              " 'uma',\n",
              " 'os',\n",
              " 'no',\n",
              " 'se',\n",
              " 'na',\n",
              " 'por',\n",
              " 'mais',\n",
              " 'as',\n",
              " 'dos',\n",
              " 'como',\n",
              " 'mas',\n",
              " 'ao',\n",
              " 'ele',\n",
              " 'das',\n",
              " 'à',\n",
              " 'seu',\n",
              " 'sua',\n",
              " 'ou',\n",
              " 'quando',\n",
              " 'muito',\n",
              " 'nos',\n",
              " 'já',\n",
              " 'eu',\n",
              " 'também',\n",
              " 'só',\n",
              " 'pelo',\n",
              " 'pela',\n",
              " 'até',\n",
              " 'isso',\n",
              " 'ela',\n",
              " 'entre',\n",
              " 'depois',\n",
              " 'sem',\n",
              " 'mesmo',\n",
              " 'aos',\n",
              " 'seus',\n",
              " 'quem',\n",
              " 'nas',\n",
              " 'me',\n",
              " 'esse',\n",
              " 'eles',\n",
              " 'você',\n",
              " 'essa',\n",
              " 'num',\n",
              " 'nem',\n",
              " 'suas',\n",
              " 'meu',\n",
              " 'às',\n",
              " 'minha',\n",
              " 'numa',\n",
              " 'pelos',\n",
              " 'elas',\n",
              " 'qual',\n",
              " 'nós',\n",
              " 'lhe',\n",
              " 'deles',\n",
              " 'essas',\n",
              " 'esses',\n",
              " 'pelas',\n",
              " 'este',\n",
              " 'dele',\n",
              " 'tu',\n",
              " 'te',\n",
              " 'vocês',\n",
              " 'vos',\n",
              " 'lhes',\n",
              " 'meus',\n",
              " 'minhas',\n",
              " 'teu',\n",
              " 'tua',\n",
              " 'teus',\n",
              " 'tuas',\n",
              " 'nosso',\n",
              " 'nossa',\n",
              " 'nossos',\n",
              " 'nossas',\n",
              " 'dela',\n",
              " 'delas',\n",
              " 'esta',\n",
              " 'estes',\n",
              " 'estas',\n",
              " 'aquele',\n",
              " 'aquela',\n",
              " 'aqueles',\n",
              " 'aquelas',\n",
              " 'isto',\n",
              " 'aquilo',\n",
              " 'estou',\n",
              " 'está',\n",
              " 'estamos',\n",
              " 'estão',\n",
              " 'estive',\n",
              " 'esteve',\n",
              " 'estivemos',\n",
              " 'estiveram',\n",
              " 'estava',\n",
              " 'estávamos',\n",
              " 'estavam',\n",
              " 'estivera',\n",
              " 'estivéramos',\n",
              " 'esteja',\n",
              " 'estejamos',\n",
              " 'estejam',\n",
              " 'estivesse',\n",
              " 'estivéssemos',\n",
              " 'estivessem',\n",
              " 'estiver',\n",
              " 'estivermos',\n",
              " 'estiverem',\n",
              " 'hei',\n",
              " 'há',\n",
              " 'havemos',\n",
              " 'hão',\n",
              " 'houve',\n",
              " 'houvemos',\n",
              " 'houveram',\n",
              " 'houvera',\n",
              " 'houvéramos',\n",
              " 'haja',\n",
              " 'hajamos',\n",
              " 'hajam',\n",
              " 'houvesse',\n",
              " 'houvéssemos',\n",
              " 'houvessem',\n",
              " 'houver',\n",
              " 'houvermos',\n",
              " 'houverem',\n",
              " 'houverei',\n",
              " 'houverá',\n",
              " 'houveremos',\n",
              " 'houverão',\n",
              " 'houveria',\n",
              " 'houveríamos',\n",
              " 'houveriam',\n",
              " 'sou',\n",
              " 'somos',\n",
              " 'são',\n",
              " 'era',\n",
              " 'éramos',\n",
              " 'eram',\n",
              " 'fui',\n",
              " 'foi',\n",
              " 'fomos',\n",
              " 'foram',\n",
              " 'fora',\n",
              " 'fôramos',\n",
              " 'seja',\n",
              " 'sejamos',\n",
              " 'sejam',\n",
              " 'fosse',\n",
              " 'fôssemos',\n",
              " 'fossem',\n",
              " 'for',\n",
              " 'formos',\n",
              " 'forem',\n",
              " 'serei',\n",
              " 'será',\n",
              " 'seremos',\n",
              " 'serão',\n",
              " 'seria',\n",
              " 'seríamos',\n",
              " 'seriam',\n",
              " 'tenho',\n",
              " 'tem',\n",
              " 'temos',\n",
              " 'tém',\n",
              " 'tinha',\n",
              " 'tínhamos',\n",
              " 'tinham',\n",
              " 'tive',\n",
              " 'teve',\n",
              " 'tivemos',\n",
              " 'tiveram',\n",
              " 'tivera',\n",
              " 'tivéramos',\n",
              " 'tenha',\n",
              " 'tenhamos',\n",
              " 'tenham',\n",
              " 'tivesse',\n",
              " 'tivéssemos',\n",
              " 'tivessem',\n",
              " 'tiver',\n",
              " 'tivermos',\n",
              " 'tiverem',\n",
              " 'terei',\n",
              " 'terá',\n",
              " 'teremos',\n",
              " 'terão',\n",
              " 'teria',\n",
              " 'teríamos',\n",
              " 'teriam']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2v4-CrlRoZD",
        "outputId": "8313c1da-8e98-4fff-c3fe-f4d2b9913cce"
      },
      "source": [
        "  # Cria uma regex que pega apenas as palavras de um texto\n",
        "  tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w+')\n",
        "  tokens = tokenizer.tokenize(corpus)\n",
        "\n",
        "\n",
        "  # transformando as palavras em minúsculas\n",
        "  nova_lista = []\n",
        "  for token in tokens:\n",
        "    # Filtrando as palavras  irrelevantes do texto e adicionando apenas as relevantes na nova lista\n",
        "    if token.lower() not in stopwords:\n",
        "      nova_lista.append(token.lower())\n",
        "  \n",
        "\n",
        "  # Conta a quantidade de vezes que a palavra se repete no texto\n",
        "  frequencia = nltk.FreqDist(nova_lista)\n",
        "  frequencia.most_common(10)\n",
        "\n",
        "  "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('jardas', 15),\n",
              " ('giants', 11),\n",
              " ('patriots', 10),\n",
              " ('manning', 10),\n",
              " ('linha', 10),\n",
              " ('bola', 7),\n",
              " ('vez', 6),\n",
              " ('zone', 6),\n",
              " ('new', 5),\n",
              " ('passe', 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chjJ1bHoTkDe"
      },
      "source": [
        "### **Usando List Comprehensions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6T9NJOYS8A5",
        "outputId": "0403f60a-0ef7-4809-b171-724b11e33e71"
      },
      "source": [
        "  # Cria uma regex que pega apenas as palavras de um texto\n",
        "  tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w+')\n",
        "  tokens = tokenizer.tokenize(corpus)\n",
        "\n",
        "\n",
        "  # transformando as palavras em minúsculas Usando List Comprehensions\n",
        "  nova_lista = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
        "  #for token in tokens:\n",
        "    # Filtrando as palavras  irrelevantes do texto e adicionando apenas as relevantes na nova lista\n",
        "  #  if token.lower() not in stopwords:\n",
        "  #    nova_lista.append(token.lower())\n",
        "  \n",
        "\n",
        "  # Conta a quantidade de vezes que a palavra se repete no texto\n",
        "  frequencia = nltk.FreqDist(nova_lista)\n",
        "  frequencia.most_common(10)\n",
        "\n",
        "  "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('jardas', 15),\n",
              " ('giants', 11),\n",
              " ('patriots', 10),\n",
              " ('manning', 10),\n",
              " ('linha', 10),\n",
              " ('bola', 7),\n",
              " ('vez', 6),\n",
              " ('zone', 6),\n",
              " ('new', 5),\n",
              " ('passe', 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}